{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Exercise 02\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Pingo\n",
    "Try to find the right answer(s) to each question on your own or in a group with your colleagues. The interactive survey will be conducted near the end of the practice class.\n",
    "\n",
    "### Welche der folgenden Aussagen zu Precision/Recall sind korrekt?\n",
    "1. Ein Model das immer Klasse A vorraussagt hat eine Precision für Klasse A von 100%\n",
    "2. Ein Model das immer Klasse A vorraussagt hat eine Precision für Klasse A von 0%\n",
    "3. Ein Model das immer Klasse A vorraussagt hat einen Recall für Klasse A von 100%\n",
    "4. Ein Model das immer Klasse A vorraussagt hat einen Recall für Klasse A von 0%\n",
    "5. F1 ist eine Kombination aus Precision und Recall\n",
    "6. F1 ist eine Kombination aus Precision, Recall und Accuracy\n",
    "\n",
    "### Welche der gegebenen Aktivierungsfunktionen sind differenzierbar?\n",
    "1. Unit Step (Threshold)\n",
    "2. Sigmoid\n",
    "3. tanh\n",
    "4. ReLU\n",
    "5. Softplus\n",
    "\n",
    "### Der Cross-Entropy Loss...\n",
    "1. ...hat sein Minimum bei 0\n",
    "2. ...ist die natürliche Wahl bei einer Softmax Aktivierungsfunktion\n",
    "3. ...basiert auf der Distanz zwischen zwei Wahrscheinlichkeitsverteilungen\n",
    "4. ... sollte bei Multiklassen Problemen zugunsten des Square Losses nicht genutzt werden\n",
    "\n",
    "### Ein Perceptron kann...\n",
    "1. ...Daten durch ein Hyperplane trennen\n",
    "2. ...das OR-Problem lösen\n",
    "3. ...das AND-Problem lösen\n",
    "4. ...das XOR-Problem lösen\n",
    "5. ...alle linear separierbaren Mengen entscheiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Machine Learning Fundamentals\n",
    "### Task 2.1 Datasets\n",
    "State two benefits / useful applications of a development dataset.\n",
    "\n",
    "#### Solution\n",
    "* hyperparameter optimization\n",
    "* early stopping\n",
    "* (avoid overfitting the test set)\n",
    "\n",
    "### Task 2.2 Evaluation Measures\n",
    "Precision, recall and F1 measure are typical measures for evaluating the results of machine learning systems.\n",
    "Assume you built a simple POS-tagger which only operates on the three tags `NN`, `VB` and `ADJ`.\n",
    "\n",
    "a) Compute precision, recall and F1 measure for each individual class based on the following confusion matrix:\n",
    "\n",
    "|              |       |      |      |       |\n",
    "|-------------:|-------|-----:|-----:|------:|\n",
    "| prediction → |       | `NN` | `VB` | `ADJ` |\n",
    "| true class ↓ | `NN`  |   25 |    5 |     1 |\n",
    "|              | `VB`  |    2 |   15 |    12 |\n",
    "|              | `ADJ` |    1 |    6 |     0 |\n",
    "\n",
    "Hint:\n",
    "\n",
    "For $n$ classes and a confusion matrix $C \\in \\mathcal{R}^{n \\times n}$, the evaluation measures are defined for class $i$ by:\n",
    "\\begin{align*}\n",
    "    P_i = \\frac{C_{i,i}}{\\sum_{j=1}^n C_{j,i}} \\\\\n",
    "    R_i = \\frac{C_{i,i}}{\\sum_{j=1}^n C_{i,j}} \\\\\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{equation}\n",
    "    F1 = \\frac{2 \\cdot P \\cdot R}{P + R} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "b) Compute the micro-/macro-averaged variant of precision, recall and F1 across all classes.\n",
    "\n",
    "Hint:\n",
    "\\begin{align*}\n",
    "    P_\\text{micro} = R_\\text{micro} = \\frac{\\sum_{i=1}^n C_{i,i}}{\\sum_{i,j \\text{ for } i \\neq j}^n C_{i,j}} \\\\\n",
    "    P_\\text{macro} = \\frac{1}{n} \\cdot \\sum_i^n P_i \\\\\n",
    "    R_\\text{macro} = \\frac{1}{n} \\cdot \\sum_i^n R_i \\\\\n",
    "\\end{align*}\n",
    "\n",
    "c) Explain the difference between the micro- and macro-averaged variants. Which variant is better suited for which occasion?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "a)\n",
    "\\begin{align*}\n",
    "    P_\\text{NN} &= \\frac{25}{25+3} &= 0.89 \\\\\n",
    "    R_\\text{NN} &= \\frac{25}{25+6} &= 0.81 \\\\\n",
    "    F1_\\text{NN} &= \\frac{2 \\cdot 0.89 \\cdot 0.81}{0.89 + 0.81} &= 0.85 \\\\[0.75em]\n",
    "    P_\\text{VB} &= \\frac{15}{15+11} &= 0.58 \\\\\n",
    "    R_\\text{VB} &= \\frac{15}{15+14} &= 0.52 \\\\\n",
    "    F1_\\text{VB} &= \\frac{2 \\cdot 0.58 \\cdot 0.52}{0.58 + 0.52} &= 0.55 \\\\[0.75em]\n",
    "    P_\\text{ADJ} &= \\frac{0}{0+13} &= 0.00 \\\\\n",
    "    R_\\text{ADJ} &= \\frac{0}{0+7} &= 0.00 \\\\\n",
    "    F1_\\text{ADJ} &= \\text{undefined} \\\\    \n",
    "\\end{align*}\n",
    "\n",
    "b)\n",
    "\n",
    "\\begin{align*}\n",
    "    P_\\text{micro} &= \\frac{25+15+0}{25+3+15+11+0+13} &= 0.60 \\\\\n",
    "    R_\\text{micro} &= \\frac{25+15+0}{25+6+15+14+0+7} &= 0.60 \\\\\n",
    "    F1_\\text{micro} &= \\frac{2 \\cdot 0.60 \\cdot 0.60}{0.60 + 0.60} &= 0.60 \\\\[0.75em]\n",
    "    P_\\text{macro} &= \\frac{1}{3} \\cdot \\left(0.89 + 0.58 + 0.00\\right) &= 0.49 \\\\\n",
    "    R_\\text{macro} &= \\frac{1}{3} \\cdot \\left(0.81 + 0.52 + 0.00\\right) &= 0.44 \\\\\n",
    "    F1_\\text{macro} &= \\frac{2 \\cdot 0.49 \\cdot 0.44}{0.49 + 0.44} &= 0.46 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "c)\n",
    "* micro-averaging: averages on the level of test instances; classes with large number of instance have strong influence on result\n",
    "* macro-averaging: averages on the class level; classes with a small number of instances keep their influence and do not get lost in the\n",
    "\n",
    "* which one for which occasion: micro, when performance on large classes is of importance; macro, when performance on small classes is of importance -> both are useful, there is no clear winner/loser\n",
    "\n",
    "### Task 2.3 Meaningful Research\n",
    "\n",
    "> It is the year 2015. Gina researches part-of-speech (POS) tagging and just reached a new state-of-the-art on the Penn Treebank corpus (which was created in 1992). Her Bidirectional Long-Short Term Memory Conditional Random Field Model (BiLSTM-CRF) reaches 97.55% accuracy, which improves over a Support Vector Machine (SVM) baseline from the year 2004 by 0.39%. She plans to submit a paper on her model to ACL 2015.\n",
    "\n",
    "Which issues do you spot in her research approach?\n",
    "\n",
    "#### Solution\n",
    "* Issues:\n",
    "\t* model seems unnecessarily complex\n",
    "\t* improvement might not be significant\n",
    "        * interesting fact: human accuracy for POS-tagging on the Penn Treebank dataset is around 3%, i.e. results above 97% are just guessed correctly\n",
    "\t* repeatedly publishing papers to report improvements on the same test dataset is also a form of overfitting\n",
    "\t\t* in theory, modifying a system after testing it dataset set is overfitting, if the same dataset is used again for testing\n",
    "\t\t* in theory, a user study on a downstream task is necessary to show the actual impact of research in NLP\n",
    "\t\t\t* but that takes time and costs money\n",
    "* Recommendations:\n",
    "\t* Gina should test her model and the SVM on entirely new, unseen test data (if available)\n",
    "\t* or even better, Gina should conduct a user study to evaluate if humans notice the improvement of her model in a user study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Pingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
