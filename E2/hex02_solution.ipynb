{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DL4NLP SS17 Home Exercise 02\n",
    "----------------------------------\n",
    "**Due until Tuesday, 02.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 1 Mandatory Video (1P)\n",
    "Watch [this week's mandatory video](https://www.youtube.com/watch?v=GUtlrDbHhJM&t=277s&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=5) and answer the following question: Which six activation functions are discussed during the talk?\n",
    "\n",
    "Hint: They are shown on a summary slide among the first 30 slides.\n",
    "\n",
    "### Solution\n",
    "Slide 5-29: Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, EL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 2 Sentiment Polarity in Movie Reviews (9P)\n",
    "The movie-review dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/) consists of movie reviews labeled with sentiment polarity (i.e. \"positive review\" or \"negative review\"). Your task is to implement a variation of the perceptron from last week which learns to identify the sentiment in a review.\n",
    "\n",
    "#### Data\n",
    "In the `hex02_data` archive, you can find a training, development and test dataset. Each line in these datasets has three entries, separated by a tab character (`\\t`). The first is the movie review (available only for reference), the second is the sentiment label (`POS`itive or `NEG`ative). To facilitate the task, the third entry is a 100-dimensional vector representing the review (we'll cover in later lectures on *word embeddings* how this sentence representation has been generated).\n",
    "\n",
    "#### Perceptron\n",
    "* As the loss function, choose square-loss:\n",
    "\\begin{equation}\n",
    "    L = \\sum_{j=1}^N \\ell(\\mathbf{x}_j, y) = \\sum_{j=1}^N (\\sigma(\\mathbf{x}_j \\cdot \\mathbf{w}) - y_j)^2\n",
    "\\end{equation}\n",
    "* For the activation function, use the sigmoid function.\n",
    "\n",
    "* For the weight update rule, use the following mini-batch stochastic gradient descent formula:\n",
    "\\begin{equation}\n",
    "    w' \\leftarrow w - \\frac{\\alpha}{|\\mathcal{T}'|} \\cdot \\sum_{(\\mathbf{x},y)\\in\\mathcal{T}'} \\Bigl(\\sigma(\\mathbf{x} \\cdot \\mathbf{w}) - y\\Bigr) \\cdot \\sigma'(\\mathbf{x} \\cdot \\mathbf{w}) \\cdot x^T\n",
    "\\end{equation}\n",
    "Reminder: $\\mathcal{T'}$ is a mini-batch; a random subset of the whole training data $\\mathcal{T}$. A typical way of implementing random mini-batches is to randomly shuffle the whole training dataset before each epoch, then divide the training dataset into batches of size $|\\mathcal{T'}|$. Consider setting the numpy random seed for reproducible results.\n",
    "\n",
    "* Use the 100-dimensional vectors from the datasets for the input vectors $\\mathbf{x}$. Encode the corresponding label as $y=1$ for `POS` and $y=0$ for `NEG` (i.e. according to the co-domain of the sigmoid activation function). **Append a trailing 1 to each input vector $\\mathbf{x}$ for the bias (cf. lecture 01, slide 92).**\n",
    "\n",
    "* Initialize the weight vector via `w=np.random.normal(0,1,(N,1))`, where $N$ is the dimensionality of your input data.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "\n",
    "\n",
    "### Task 2.1 Dataset reader (1P)\n",
    "Implement a reader for the dataset files which returns the input vectors $\\mathbf{x}$ and labels $y$ as numpy arrays. The shape and number of returned arrays is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "label_pos = 1\n",
    "label_neg = 0\n",
    "\n",
    "data_dim = 100\n",
    "data_dim_with_bias = data_dim + 1\n",
    "\n",
    "def read_dataset(src):\n",
    "    \"\"\"Reads a dataset from the specified path and returns input vectors and labels in an array of shape (n, 101).\"\"\"\n",
    "    with open(src, 'r') as src_file:\n",
    "        # preallocate memory for the data\n",
    "        num_lines = sum(1 for line in src_file)\n",
    "        data = np.empty((num_lines, data_dim_with_bias), dtype=np.float16)\n",
    "        labels = np.empty((num_lines, 1), dtype=np.float16)\n",
    "\n",
    "        # reset the file pointer to the beginning of the file\n",
    "        src_file.seek(0)\n",
    "        for i, line in enumerate(src_file):\n",
    "            _, str_label, str_vec = line.split('\\t')\n",
    "            labels[i] = label_pos if str_label.split('=')[1] == \"POS\" else label_neg\n",
    "            data[i,:data_dim] = [float(f) for f in str_vec.split()]\n",
    "            data[i,data_dim] = 1\n",
    "    return data, labels\n",
    "\n",
    "def get_dataset(src_folder, name=\"train\"):\n",
    "    path = os.path.join(src_folder, \"rt-polarity.{}.vecs\".format(name))\n",
    "    return read_dataset(path)\n",
    "\n",
    "def get_random_batches(X, y, batch_size):\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    # when using array_split for 100 datapoints and batch size 33 one would get batches [33, 33, 33, 1]\n",
    "    X_batches = np.array_split(X, len(y)//batch_size)\n",
    "    y_batches = np.array_split(y, len(y)//batch_size)\n",
    "    return X_batches, y_batches\n",
    "\n",
    "# load the data\n",
    "train_x, train_y, = get_dataset(\"DATA\", \"train\")\n",
    "dev_x, dev_y = get_dataset(\"DATA\", \"dev\")\n",
    "test_x, test_y = get_dataset(\"DATA\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Numpy implementation (5P)\n",
    "\n",
    "a) Implement the perceptron stated above only using numpy. Include a method which computes the square loss and the accuracy of the model, given a dataset and a weight vector `w`. (3P)\n",
    "\n",
    "Hint: In order to compute the accuracy, you need to find a meaningful way to interpret your perceptron's prediction $\\sigma(\\mathbf{x} \\cdot \\mathbf{w})$ for a given test input $\\mathbf{x}$ and trained weights $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "test_every_ith_epoch = 20\n",
    "\n",
    "def sigmoid(v):\n",
    "      return 1.0 / (1+np.exp(-v))\n",
    "\n",
    "def train_np(train_x, train_y, w, epochs, batch_size, learning_rate):\n",
    "\n",
    "    def epoch(train_x, train_y, w_init, batch_size, learning_rate):\n",
    "        w = w_init\n",
    "        train_x_batches, train_y_batches = get_random_batches(train_x, train_y, batch_size)\n",
    "        for X_batch, y_batch in zip(train_x_batches, train_y_batches):\n",
    "            grad = 0\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                sigxw = sigmoid(np.dot(x, w))\n",
    "                grad += (sigxw-y) * sigxw * (1 - sigxw) * x\n",
    "            # average the gradient\n",
    "            grad /= len(y_batch)\n",
    "            w -= learning_rate * grad\n",
    "        return w\n",
    "  \n",
    "    # keep track of losses on the train and dev sets for the plot in Task 2.2 b)\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    bar = pb.ProgressBar(max_value=epochs)\n",
    "    for i in bar(range(epochs)):\n",
    "        w = epoch(train_x, train_y, w, batch_size, learning_rate)\n",
    "    \n",
    "        # compute the loss every i epochs\n",
    "        if not i%test_every_ith_epoch:\n",
    "            loss, _ = test_np(dev_x, dev_y, w)\n",
    "            losses_dev.append(loss)      \n",
    "            loss, _ = test_np(train_x, train_y, w)\n",
    "            losses_train.append(loss)\n",
    "    return w, losses_train, losses_dev\n",
    "\n",
    "\n",
    "def test_np(test_x, test_y, w):\n",
    "    predictions = [sigmoid(np.dot(x, w)) for x in test_x]\n",
    "    predictions_discrete = [np.rint(pred) for pred in predictions]\n",
    "    \n",
    "    n = len(test_y)    \n",
    "    mean_square_loss = sum([(pred - y)**2 for pred, y in zip(predictions, test_y)])/n\n",
    "    accuracy = sum([pred == y for pred, y in zip(predictions_discrete, test_y)])/n\n",
    "\n",
    "    return mean_square_loss, accuracy\n",
    "\n",
    "def run_numpy(epochs=100, batch_size=10, learning_rate=0.01):\n",
    "    np.random.seed(seed=42)\n",
    "    np.seterr(all='ignore')\n",
    "\n",
    "    w_init = np.random.normal(0, 1, (data_dim_with_bias))\n",
    "    w,losses_train,losses_dev = train_np(train_x, train_y, w_init, epochs, batch_size, learning_rate)\n",
    "\n",
    "    #print results on dev and test\n",
    "    loss, accuracy = test_np(dev_x, dev_y, w)\n",
    "    print(\"Loss on dev after {} epochs: {}, accuracy: {}\".format(epochs, loss, accuracy))\n",
    "    loss, accuracy = test_np(test_x, test_y, w)\n",
    "    print(\"Loss on test after {} epochs: {}, accuracy: {}\".format(epochs, loss, accuracy))\n",
    "\n",
    "    # return values for the plot\n",
    "    return losses_train, losses_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Train your perceptron on the training data and observe its accuracy on the **development** set. Start with batch size $|\\mathcal{T}'| = 10$, learning rate $\\alpha = 0.01$ and 100 epochs. Experiment with different values for these three hyperparameters. Can you find a configuration which beats 70% accuracy on the development set? Report your best configuration and both the loss and accuracy it reaches on the **development and test** sets. (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:16\n",
      "  0% (1 of 500) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:01:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (500 of 500) |######################################################################| Elapsed Time: 0:00:40 Time: 0:00:40  0% (4 of 500) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 500 epochs: [ 0.27939337], accuracy: [ 0.71732333]\n",
      "Loss on test after 500 epochs: [ 0.29846779], accuracy: [ 0.69981238]\n"
     ]
    }
   ],
   "source": [
    "# initial parameters\n",
    "_,_ = run_numpy()\n",
    "# console output:\n",
    "# Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
    "# Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n",
    "\n",
    "# better parameters\n",
    "_,_ = run_numpy(epochs=500, batch_size=25, learning_rate=0.05)\n",
    "# console output:\n",
    "# Loss on dev after 500 epochs: [ 0.27939337], accuracy: [ 0.71732333]\n",
    "# Loss on test after 500 epochs: [ 0.29846779], accuracy: [ 0.69981238]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create a plot similar to the one from lecture 02, slide 18, i.e. plot the loss on the training set and the development set vs. the number of training epochs. For this purpose, run your perceptron with batch size $|\\mathcal{T}'| = 1$ and learning rate $\\alpha = 0.001$ for a large number of epochs (>2500). Which number of epochs is reasonable? Why does the loss on the development set **not** increase over time, contrary to the figure from the lecture? (1P)\n",
    "\n",
    "Hint: For comparable results in the plot, normalize the loss by the number of instances in the training/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2500 of 2500) |####################################################################| Elapsed Time: 0:06:43 Time: 0:06:43  0% (1 of 2500) |                                                                        | Elapsed Time: 0:00:00 ETA: 0:10:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 2500 epochs: [ 0.27392119], accuracy: [ 0.72357724]\n",
      "Loss on test after 2500 epochs: [ 0.3056598], accuracy: [ 0.6916823]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0327858630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses_train, losses_dev = run_numpy(epochs=2500, batch_size=1, learning_rate=0.001)\n",
    "\n",
    "x_vals = [test_every_ith_epoch*i for i in range(len(losses_train))]\t\n",
    "plt.plot(x_vals, losses_train, '-r', label='train loss')\n",
    "plt.plot(x_vals, losses_dev, '-b', label='dev loss')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Improvement in terms of loss on the development set stagnates after roughly 800 epochs. A good early stopping point could therefore be 800 epochs. The loss on the development set does not start to increase for late epochs because, roughly speaking, the data is more complex than the model. The linear decision boundary of our perceptron is not expressive enough to overfit noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 TensorFlow (3P)\n",
    "[Install TensorFlow](https://www.tensorflow.org/install/) on your machine and reimplement the perceptron with TensorFlow. Using the initial hyperparameter configuration mentioned in Task 2.2 b), [compare the runtime](http://stackoverflow.com/a/7370824) for training your TensorFlow and your numpy perceptrons for an average of 10 executions. Which one runs faster on your machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import progressbar as pb\n",
    "import time\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, learning_rate):\n",
    "        # set up the model\n",
    "        self.X = tf.placeholder(tf.float16, shape=(None, data_dim_with_bias))\n",
    "        self.y = tf.placeholder(tf.float16, shape=(None, 1))\n",
    "        self.w = tf.get_variable(\"weights\",\n",
    "                            shape=(data_dim_with_bias, 1),\n",
    "                            dtype=tf.float16,\n",
    "                            initializer=tf.random_normal_initializer())\n",
    "        self.perceptron = tf.sigmoid(tf.matmul(self.X, self.w))\n",
    "        self.loss = tf.losses.mean_squared_error(self.y, self.perceptron)\n",
    "        self.sgd = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "def train_tf(sess, model, train_x, train_y, epochs, batch_size):    \n",
    "    bar = pb.ProgressBar(max_value=epochs)\n",
    "    for i in bar(range(epochs)):\n",
    "        train_x_batches, train_y_batches = get_random_batches(train_x, train_y, batch_size)\n",
    "        for X_batch, y_batch in zip(train_x_batches, train_y_batches):\n",
    "            _, w_val = sess.run([model.sgd, model.w], feed_dict={model.X:X_batch, model.y:y_batch})\n",
    "    \n",
    "def test_tf(sess, model, test_x, test_y):\n",
    "    loss, predictions = sess.run([model.loss, model.perceptron], feed_dict={model.X:test_x, model.y:test_y})\n",
    "    accuracy = np.sum(np.equal(test_y, np.rint(predictions))) / len(test_y)\n",
    "    return loss, accuracy\n",
    "                  \n",
    "def run_tensorflow(epochs=100, batch_size=10, learning_rate=0.01):\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    # set up the model\n",
    "    model = Model(learning_rate)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_tf(sess, model, train_x, train_y, epochs, batch_size)\n",
    "        \n",
    "        #print results on dev and test\n",
    "        loss, accuracy = test_tf(sess, model, dev_x, dev_y)\n",
    "        print(\"Loss on dev after {} epochs: {}, accuracy: {}\".format(epochs, loss, accuracy))\n",
    "        loss, accuracy = test_tf(sess, model, test_x, test_y)\n",
    "        print(\"Loss on test after {} epochs: {}, accuracy: {}\".format(epochs, loss, accuracy))\n",
    "    \n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:08 Time: 0:00:08  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:10\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:11\n",
      "  1% (1 of 100) |                                                                         | Elapsed Time: 0:00:00 ETA: 0:00:17"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:09 Time: 0:00:09  3% (3 of 100) |##                                                                       | Elapsed Time: 0:00:00 ETA: 0:00:13\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: [ 0.3056598], accuracy: [ 0.69043152]\n",
      "Loss on test after 100 epochs: [ 0.32520324], accuracy: [ 0.66979362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:27 Time: 0:00:27  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:27\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.29010888934135437, accuracy: 0.7066916823014384\n",
      "Loss on test after 100 epochs: 0.30032747983932495, accuracy: 0.6954346466541589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:28 Time: 0:00:28  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:25\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.30025187134742737, accuracy: 0.6985616010006254\n",
      "Loss on test after 100 epochs: 0.30845949053764343, accuracy: 0.6885553470919324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:27 Time: 0:00:27  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:28\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.5084272623062134, accuracy: 0.4915572232645403\n",
      "Loss on test after 100 epochs: 0.49211055040359497, accuracy: 0.5078173858661663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:27 Time: 0:00:27  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:30\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.29484695196151733, accuracy: 0.7035647279549718\n",
      "Loss on test after 100 epochs: 0.30881547927856445, accuracy: 0.6904315196998124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:25 Time: 0:00:25  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:22\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.31006625294685364, accuracy: 0.6866791744840526\n",
      "Loss on test after 100 epochs: 0.31565889716148376, accuracy: 0.6804252657911195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:24 Time: 0:00:24  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:23\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.5059831738471985, accuracy: 0.49405878674171355\n",
      "Loss on test after 100 epochs: 0.49456387758255005, accuracy: 0.5053158223889931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:28 Time: 0:00:28  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:23\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.2914016842842102, accuracy: 0.7048155096935584\n",
      "Loss on test after 100 epochs: 0.3072618544101715, accuracy: 0.6898061288305191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:28 Time: 0:00:28  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:27\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.3085114657878876, accuracy: 0.6904315196998124\n",
      "Loss on test after 100 epochs: 0.3167217969894409, accuracy: 0.6804252657911195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:25 Time: 0:00:25  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:25\n",
      "N/A% (0 of 100) |                                                                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.2966122031211853, accuracy: 0.7023139462163852\n",
      "Loss on test after 100 epochs: 0.2940548360347748, accuracy: 0.7035647279549718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################################################################| Elapsed Time: 0:00:27 Time: 0:00:27  2% (2 of 100) |#                                                                        | Elapsed Time: 0:00:00 ETA: 0:00:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on dev after 100 epochs: 0.28633901476860046, accuracy: 0.7098186366479049\n",
      "Loss on test after 100 epochs: 0.2988188564777374, accuracy: 0.6979362101313321\n",
      "Average training time (seconds): {'run_tensorflow': 27.315894603729248, 'run_numpy': 9.2817548751831058}\n"
     ]
    }
   ],
   "source": [
    "def speed_comparison(runs=10):\n",
    "    timings_by_system = {}\n",
    "    for train_system in [run_numpy, run_tensorflow]:\n",
    "        timings = np.empty((runs))\n",
    "        for r in range(runs):\n",
    "            t0 = time.time()\n",
    "            train_system(epochs=100, batch_size=10, learning_rate=0.01)\n",
    "            t1 = time.time()\n",
    "            timings[r] = t1 - t0\n",
    "        timings_by_system[train_system.__name__] = np.mean(timings)\n",
    "    print(\"Average training time (seconds): \" + str(timings_by_system))\n",
    "    \n",
    "speed_comparison()\n",
    "# console output:\n",
    "# Average training time (seconds): {'run_numpy': 8.8820782661437985, 'run_tensorflow': 22.123092341423035}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
