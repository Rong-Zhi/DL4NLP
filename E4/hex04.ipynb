{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 04\n",
    "----------------------------------\n",
    "**Due until Tuesday, 16.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Mandatory Paper (2P)\n",
    "\n",
    "Read [this week's mandatory paper](https://arxiv.org/abs/1301.3781) and answer the following questions: What is the main difference between CBOW and Skip-Gram in comparison to previous models? What is the benefit of this difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Embeddings (8P)\n",
    "\n",
    "### Task 2.0 Setup (1P)\n",
    "For this home exercise, you will need:\n",
    "* [the gensim python library](https://radimrehurek.com/gensim)\n",
    "* [the scipy python library](https://www.scipy.org/scipylib/index.html)\n",
    "* [the binary pretrained 300-dimensional word2vec embeddings from Google](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing), extracted\n",
    "* [the WS-353 dataset](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/), a classic word similarity dataset\n",
    "\n",
    "### Task 2.1 Word Similarity (3P)\n",
    "In this task, you will evaluate how well the pretrained word2vec embeddings perform in the word similarity task on the WS-353 dataset. Gensim provides such a functionality with the `evaluate_word_pairs` method, but we will take the manual route in this task.\n",
    "\n",
    "a) In the `combined.tab` file from the WS-353 dataset, each row contains two words and a mean similarity score assigned by humans. The three values are separated by tab characters (`\\t`). Write a python method which reads the dataset into an appropriate format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b) Load the pretrained binary word2vec embeddings with gensim, then compute the pairwise word similarity between each word pair using gensim's `similarity` method.\n",
    "\n",
    "Hint: https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) is a typical choice for measuring the ranking of two variables. Compute the coefficient between the values assigned by humans and your results from b) using scipy. Explain in two sentences what your resulting coefficient means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Gamified Word Intrusion (4P)\n",
    "\"Word intrusion\" is a task used by [Faruqui et al. 2015](https://arxiv.org/pdf/1506.02004.pdf) for intrinsic evaluation of word embeddings. The idea is: \"In one instance of the experiment, a human judge is presented with five words in random order and asked to select the intruder.\" The intruder is a word unrelated to the four other words.\n",
    "\n",
    "In this task you will not evaluate embeddings via word intrusion, but you will rather use embeddings to write a \"game\" based on word embeddings, where a human has to identify the intruder in a set of five words.\n",
    "\n",
    "Write a python script which repeats the following steps:\n",
    "1. Find four words which are similar to each other according to the pretrained word2vec embeddings, and one intruder word.\n",
    "2. Print the five words in random order, then query a human (i.e. yourself) to spot the intruder. The python method `input` might be of use here.\n",
    "3. Print the accuracy currently reached by the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
