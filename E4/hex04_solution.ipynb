{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 04\n",
    "----------------------------------\n",
    "**Due until Tuesday, 16.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Mandatory Paper (2P)\n",
    "\n",
    "What is the main difference between CBOW and Skip-Gram in comparison to previous models? What is the benefit of this difference?\n",
    "\n",
    "#### Solution\n",
    "The main difference is the absence of a non-linear hidden layer. It improves the learning speed and allows the training on much more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2 Embeddings (8P)\n",
    "\n",
    "### Task 2.0 Setup (1P)\n",
    "For this home exercise, you will need:\n",
    "* [the gensim python library](https://radimrehurek.com/gensim)\n",
    "* [the scipy python library](https://www.scipy.org/scipylib/index.html)\n",
    "* [the binary pretrained 300-dimensional word2vec embeddings from Google](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing), extracted\n",
    "* [the WS-353 dataset](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/), a classic word similarity dataset\n",
    "\n",
    "### Task 2.1 Word Similarity (3P)\n",
    "In this task, you will evaluate how well the pretrained word2vec embeddings perform in the word similarity task on the WS-353 dataset. Gensim provides such a functionality with the `evaluate_word_pairs` method, but we will take the manual route in this task.\n",
    "\n",
    "a) In the `combined.tab` file from the WS-353 dataset, each row contains two words and a mean similarity score assigned by humans. The three values are separated by tab characters (`\\t`). Write a python method which reads the dataset into an appropriate format.\n",
    "\n",
    "b) Load the pretrained binary word2vec embeddings with gensim, then compute the pairwise word similarity between each word pair using gensim's `similarity` method.\n",
    "\n",
    "Hint: https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "c) [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) is a typical choice for measuring the ranking of two variables. Compute the coefficient between the values assigned by humans and your results from b) using scipy. Explain in two sentences what your resulting coefficient means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# a)\n",
    "def read_ws353(path):\n",
    "    with open(path, 'r') as file:\n",
    "        \n",
    "        # skip the first line\n",
    "        file = iter(file)\n",
    "        next(file)  \n",
    "\n",
    "        word_pairs = []\n",
    "        gold_labels = []\n",
    "        for line in file:\n",
    "            w1, w2, human = line.split(\"\\t\")\n",
    "            word_pairs.append((w1, w2))\n",
    "            gold_labels.append(float(human))\n",
    "    return word_pairs, gold_labels\n",
    "word_pairs, gold_labels = read_ws353(\"/var/data/wordsim353/combined.tab\")\n",
    "\n",
    "# b)\n",
    "embeddings_path = \"/var/data/word2vec/GoogleNews-vectors-negative300.bin\"\n",
    "print(\"Started reading embeddings...\")\n",
    "word_vectors = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "print(\"Embeddings read.\")\n",
    "prediction =[word_vectors.similarity(w1, w2) for w1, w2 in word_pairs]\n",
    "\n",
    "# c)\n",
    "spearmanr(gold_labels, prediction)\n",
    "\n",
    "# console output:\n",
    "# SpearmanrResult(correlation=0.70001664862721935, pvalue=2.8686666605142199e-53)\n",
    "\n",
    "# explanation:\n",
    "#   The coefficient is positive, which generally shows that w2v embedding similarity produces\n",
    "#   higher values for words of higher similarity. Since 0.7 is relatively close to 1.0 (the\n",
    "#   maximum possible coefficient), the performance can be considered quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Gamified Word Intrusion (4P)\n",
    "\"Word intrusion\" is a task used by [Faruqui et al. 2015](https://arxiv.org/pdf/1506.02004.pdf) for intrinsic evaluation of word embeddings. The idea is: \"In one instance of the experiment, a human judge is presented with five words in random order and asked to select the intruder.\" The intruder is a word unrelated to the four other words.\n",
    "\n",
    "In this task you will not evaluate embeddings via word intrusion, but you will rather use embeddings to write a \"game\" based on word embeddings, where a human has to identify the intruder in a set of five words.\n",
    "\n",
    "Write a python script which repeats the following steps:\n",
    "1. Find four words which are similar to each other according to the pretrained word2vec embeddings, and one intruder word.\n",
    "2. Print the five words in random order, then query a human (i.e. yourself) to spot the intruder. The python method `input` might be of use here.\n",
    "3. Print the accuracy currently reached by the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "vocab_list = list(word_vectors.vocab)\n",
    "correct = 0\n",
    "tries = 0\n",
    "while True:\n",
    "    # pick two random words; one \"normal\" one and the intruder, then find three similar \"normal\" words\n",
    "    seed_positive, seed_negative = random.sample(vocab_list, 2)\n",
    "    more_positive = word_vectors.most_similar_cosmul(positive=[seed_positive], negative=[seed_negative], topn=3)\n",
    "          \n",
    "    # create a list of words and insert the intruder at a random position\n",
    "    choices = [tup[0] for tup in more_positive]\n",
    "    choices.append(seed_positive)\n",
    "    intruder_pos = random.randint(0, len(choices))\n",
    "    choices.insert(intruder_pos, seed_negative)\n",
    "    \n",
    "    # query the user\n",
    "    print(\"{}\\nWhere's the intruder?\".format(\" \".join(choices)))\n",
    "    if int(input())==intruder_pos:\n",
    "        correct += 1\n",
    "    tries += 1\n",
    "    print(\"Current accuracy: {}\\n\".format(correct/tries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
