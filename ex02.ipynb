{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Exercise 02\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Pingo\n",
    "Try to find the right answer(s) to each question on your own or in a group with your colleagues. The interactive survey will be conducted near the end of the practice class.\n",
    "\n",
    "### Welche der folgenden Aussagen zu Precision/Recall sind korrekt?\n",
    "1. Ein Model das immer Klasse A vorraussagt hat eine Precision für Klasse A von 100%\n",
    "2. Ein Model das immer Klasse A vorraussagt hat eine Precision für Klasse A von 0%\n",
    "3. Ein Model das immer Klasse A vorraussagt hat einen Recall für Klasse A von 100%\n",
    "4. Ein Model das immer Klasse A vorraussagt hat einen Recall für Klasse A von 0%\n",
    "5. F1 ist eine Kombination aus Precision und Recall\n",
    "6. F1 ist eine Kombination aus Precision, Recall und Accuracy\n",
    "\n",
    "### Welche der gegebenen Aktivierungsfunktionen sind differenzierbar?\n",
    "1. Unit Step (Threshold)\n",
    "2. Sigmoid\n",
    "3. tanh\n",
    "4. ReLU\n",
    "5. Softplus\n",
    "\n",
    "### Der Cross-Entropy Loss...\n",
    "1. ...hat sein Minimum bei 0\n",
    "2. ...ist die natürliche Wahl bei einer Softmax Aktivierungsfunktion\n",
    "3. ...basiert auf der Distanz zwischen zwei Wahrscheinlichkeitsverteilungen\n",
    "4. ... sollte bei Multiklassen Problemen zugunsten des Square Losses nicht genutzt werden\n",
    "\n",
    "### Ein Perceptron kann...\n",
    "1. ...Daten durch ein Hyperplane trennen\n",
    "2. ...das OR-Problem lösen\n",
    "3. ...das AND-Problem lösen\n",
    "4. ...das XOR-Problem lösen\n",
    "5. ...alle linear separierbaren Mengen entscheiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Machine Learning Fundamentals\n",
    "### Task 2.1 Datasets\n",
    "State two benefits / useful applications of a development dataset.\n",
    "\n",
    "### Task 2.2 Evaluation Measures\n",
    "Precision, recall and F1 measure are typical measures for evaluating the results of machine learning systems.\n",
    "Assume you built a simple POS-tagger which only operates on the three tags `NN`, `VB` and `ADJ`.\n",
    "\n",
    "a) Compute precision, recall and F1 measure for each individual class based on the following confusion matrix:\n",
    "\n",
    "|              |       |      |      |       |\n",
    "|-------------:|-------|-----:|-----:|------:|\n",
    "| prediction → |       | `NN` | `VB` | `ADJ` |\n",
    "| true class ↓ | `NN`  |   25 |    5 |     1 |\n",
    "|              | `VB`  |    2 |   15 |    12 |\n",
    "|              | `ADJ` |    1 |    6 |     0 |\n",
    "\n",
    "Hint:\n",
    "\n",
    "For $n$ classes and a confusion matrix $C \\in \\mathcal{R}^{n \\times n}$, the evaluation measures are defined for class $i$ by:\n",
    "\\begin{align*}\n",
    "    P_i = \\frac{C_{i,i}}{\\sum_{j=1}^n C_{j,i}} \\\\\n",
    "    R_i = \\frac{C_{i,i}}{\\sum_{j=1}^n C_{i,j}} \\\\\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{equation}\n",
    "    F1 = \\frac{2 \\cdot P \\cdot R}{P + R} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "b) Compute the micro-/macro-averaged variant of precision, recall and F1 across all classes.\n",
    "\n",
    "Hint:\n",
    "\\begin{align*}\n",
    "    P_\\text{micro} = R_\\text{micro} = \\frac{\\sum_{i=1}^n C_{i,i}}{\\sum_{i,j \\text{ for } i \\neq j}^n C_{i,j}} \\\\\n",
    "    P_\\text{macro} = \\frac{1}{n} \\cdot \\sum_i^n P_i \\\\\n",
    "    R_\\text{macro} = \\frac{1}{n} \\cdot \\sum_i^n R_i \\\\\n",
    "\\end{align*}\n",
    "\n",
    "c) Explain the difference between the micro- and macro-averaged variants. Which variant is better suited for which occasion?\n",
    "\n",
    "### Task 2.3 Meaningful Research\n",
    "\n",
    "> It is the year 2015. Gina researches part-of-speech (POS) tagging and just reached a new state-of-the-art on the Penn Treebank corpus (which was created in 1992). Her Bidirectional Long-Short Term Memory Conditional Random Field Model (BiLSTM-CRF) reaches 97.55% accuracy, which improves over a Support Vector Machine (SVM) baseline from the year 2004 by 0.39%. She plans to submit a paper on her model to ACL 2015.\n",
    "\n",
    "Which issues do you spot in her research approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
