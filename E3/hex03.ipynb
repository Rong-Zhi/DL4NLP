{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 03\n",
    "----------------------------------\n",
    "**Due until Tuesday, 09.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Backpropagation (4P)\n",
    "\n",
    "![MLP](backprop_hex03_task.png)\n",
    "\n",
    "#### Network details\n",
    "\n",
    "The activation function used in each layer is shown above the respective layer.\n",
    "\n",
    "The network loss is square loss:\n",
    "\\begin{equation}\n",
    "    \\ell(t, y) = (t - y)^2 \\qquad \\ell'(t, y) = 2(y-t)\n",
    "\\end{equation}\n",
    "$t$ denotes the true label, $y$ denotes the network output.\n",
    "\n",
    "The weight matrices are initialized as follows:\n",
    "\\begin{equation}\n",
    "\t\\mathbf{T} = \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{U} = \\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{V} = \\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{W} = \\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Task 1.1 Backpropagation by hand (4P)\n",
    "Using pen and paper, perform one execution of backpropagation on multilayer perceptron given above. \"One execution\" includes the forward propagation step, the backwards propagation and weight updates.\n",
    "Use $\\mathbf{x} = (-1, 1)$ as the input and $\\mathbf{t} = (0,1)$ as the output. Use $\\alpha=0.75$ for the learning rate.\n",
    "\n",
    "The necessary formulas can be found on slides 11-13 [in the exercise slides](https://moodle.informatik.tu-darmstadt.de/mod/resource/view.php?id=7422). Round your results to four decimal points after each calculation.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "Please show your process of calculation. Both LaTeX solutions and scanned sheets are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Sentiment Polarity in Movie Reviews (6P)\n",
    "We revisit last week's task of identifying sentiments in movie reviews.\n",
    "\n",
    "#### Data (same as last week)\n",
    "In the `hex03_data` archive, you can find a training, development and test dataset. Each line in these datasets has three entries, separated by a tab character (`\\t`). The first is the movie review (available only for reference), the second is the sentiment label (`POS`itive or `NEG`ative). To facilitate the task, the third entry is a 100-dimensional vector representing the review (we'll cover in later lectures on *word embeddings* how this sentence representation has been generated).\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "\n",
    "\n",
    "### Task 2.0 Dataset reader (0P)\n",
    "To read the datasets, reuse your own reader from last week or copy the one from the solution provided in moodle.\n",
    "\n",
    "### Task 2.1 MLP in TensorFlow (2P)\n",
    "\n",
    "Implement a multilayer perceptron in TensorFlow. Try to keep it as configurable as possible for the hyperparameter optimization for task 2.2.\n",
    "\n",
    "Report loss and accuracy on the dev set for the following configuration:\n",
    "* square loss\n",
    "* 2 hidden layers, both with a dimension of 10 and using tanh activation\n",
    "* weights initialized by the standard normal distribution (mean 0, sigma 1)\n",
    "* batch size 10\n",
    "* learning rate $\\alpha = 0.01$\n",
    "* 100 epochs\n",
    "\n",
    "\n",
    "### Task 2.2 Hyperparameter Optimization (2P)\n",
    "In lecture 02, slide 19ff you learned about random hyperparameter optimization. Try to find the best configuration for your MLP from task 2.1 by experimenting with the following hyperparameters:\n",
    "\n",
    "* **A**: Known from the previous home exercise:\n",
    "    * batch size\n",
    "    * learning rate\n",
    "    * number of training epochs\n",
    "\n",
    "\n",
    "* **B**: New parameters:\n",
    "    * loss function: square loss, hinge loss, softmax + cross-entropy loss\n",
    "    * number of hidden layers: anything values from the interval $[0, \\infty[$\n",
    "    * hidden layer dimension: anything values from the interval $[1, \\infty[$\n",
    "    * activation function(s): sigmoid, tanh, ReLU, softplus, leaky ReLU, ELU, maxout\n",
    "    * 2-norm regularization of hidden layer weights (see lecture 03, slide 37): yes/no\n",
    "    * optimizer: SGD, Adam, AdaGrad, ...\n",
    "\n",
    "To obtain the 2P for this task, you need to experiment with all three parameters from set **A** and at least three parameters from set **B**. Report your top 10 parameter sets and their loss/accuracy on the dev set. Then, report loss and accuracy of your best parameter set on the test set.\n",
    "\n",
    "Hints:\n",
    "* Activation functions and hidden layer dimensions can also be chosen differently for each layer.\n",
    "* When varying the loss function or the activation functions, it might make sense to redefine the truth labels in the datasets from 0 for NEGative and 1 for POSitive to something else (think of the co-domain of activation functions).\n",
    "\n",
    "### Task 2.2 MLP in Keras (2P)\n",
    "[Install Keras](https://keras.io/#installation) on your machine and implement an MLP with Keras. Hardcode the initial hyperparameter configuration mentioned in Task 2.1 and report loss and accuracy on the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "## Task 1 Backpropagation (4P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\mathbf{r} = max(0,\\mathbf{x} \\mathbf{T}) =max(0, (-1, 1) \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}) = (0.61, 0.64) $ \n",
    "    \n",
    "$\\mathbf{q} = tanh(\\mathbf{r} \\mathbf{U}) = tanh((0.61, 0.64)\\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}) = tanh(-0.5879) = -0.67$\n",
    "\n",
    "$\\mathbf{p}=sig(\\mathbf{q} \\mathbf{V})=sig(-0.67\\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-1.005 & 0.5427 & 0.1608\\\\\n",
    "\\end{pmatrix}) =\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}  $\n",
    "    \n",
    "$\\mathbf{y}=sig(\\mathbf{p}\\mathbf{W}=\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-2.22 & -1.68\\end{pmatrix})=\\begin{pmatrix} 0.098 & 0.16\\end{pmatrix}$\n",
    "    \n",
    "$\\mathbf{E}=(\\mathbf{t} - \\mathbf{y})^2=\\begin{pmatrix} 0.204 & 1.198 \\end{pmatrix} $\n",
    "\n",
    "$\\frac{d \\mathbf{E}}{dy} =2(\\mathbf{t} - \\mathbf{y}) =\\begin{pmatrix} 0.408 &  2.396 \\end{pmatrix}$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
