{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 03\n",
    "----------------------------------\n",
    "**Due until Tuesday, 09.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Backpropagation (4P)\n",
    "\n",
    "![MLP](backprop_hex03_task.png)\n",
    "\n",
    "#### Network details\n",
    "\n",
    "The activation function used in each layer is shown above the respective layer.\n",
    "\n",
    "The network loss is square loss:\n",
    "\\begin{equation}\n",
    "    \\ell(t, y) = (t - y)^2 \\qquad \\ell'(t, y) = 2(y-t)\n",
    "\\end{equation}\n",
    "$t$ denotes the true label, $y$ denotes the network output.\n",
    "\n",
    "The weight matrices are initialized as follows:\n",
    "\\begin{equation}\n",
    "\t\\mathbf{T} = \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{U} = \\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{V} = \\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{W} = \\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Task 1.1 Backpropagation by hand (4P)\n",
    "Using pen and paper, perform one execution of backpropagation on multilayer perceptron given above. \"One execution\" includes the forward propagation step, the backwards propagation and weight updates.\n",
    "Use $\\mathbf{x} = (-1, 1)$ as the input and $\\mathbf{t} = (0,1)$ as the output. Use $\\alpha=0.75$ for the learning rate.\n",
    "\n",
    "The necessary formulas can be found on slides 11-13 [in the exercise slides](https://moodle.informatik.tu-darmstadt.de/mod/resource/view.php?id=7422). Round your results to four decimal points after each calculation.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "Please show your process of calculation. Both LaTeX solutions and scanned sheets are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Sentiment Polarity in Movie Reviews (6P)\n",
    "We revisit last week's task of identifying sentiments in movie reviews.\n",
    "\n",
    "#### Data (same as last week)\n",
    "In the `hex03_data` archive, you can find a training, development and test dataset. Each line in these datasets has three entries, separated by a tab character (`\\t`). The first is the movie review (available only for reference), the second is the sentiment label (`POS`itive or `NEG`ative). To facilitate the task, the third entry is a 100-dimensional vector representing the review (we'll cover in later lectures on *word embeddings* how this sentence representation has been generated).\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "\n",
    "\n",
    "### Task 2.0 Dataset reader (0P)\n",
    "To read the datasets, reuse your own reader from last week or copy the one from the solution provided in moodle.\n",
    "\n",
    "### Task 2.1 MLP in TensorFlow (2P)\n",
    "\n",
    "Implement a multilayer perceptron in TensorFlow. Try to keep it as configurable as possible for the hyperparameter optimization for task 2.2.\n",
    "\n",
    "Report loss and accuracy on the dev set for the following configuration:\n",
    "* square loss\n",
    "* 2 hidden layers, both with a dimension of 10 and using tanh activation\n",
    "* weights initialized by the standard normal distribution (mean 0, sigma 1)\n",
    "* batch size 10\n",
    "* learning rate $\\alpha = 0.01$\n",
    "* 100 epochs\n",
    "\n",
    "\n",
    "### Task 2.2 Hyperparameter Optimization (2P)\n",
    "In lecture 02, slide 19ff you learned about random hyperparameter optimization. Try to find the best configuration for your MLP from task 2.1 by experimenting with the following hyperparameters:\n",
    "\n",
    "* **A**: Known from the previous home exercise:\n",
    "    * batch size\n",
    "    * learning rate\n",
    "    * number of training epochs\n",
    "\n",
    "\n",
    "* **B**: New parameters:\n",
    "    * loss function: square loss, hinge loss, softmax + cross-entropy loss\n",
    "    * number of hidden layers: anything values from the interval $[0, \\infty[$\n",
    "    * hidden layer dimension: anything values from the interval $[1, \\infty[$\n",
    "    * activation function(s): sigmoid, tanh, ReLU, softplus, leaky ReLU, ELU, maxout\n",
    "    * 2-norm regularization of hidden layer weights (see lecture 03, slide 37): yes/no\n",
    "    * optimizer: SGD, Adam, AdaGrad, ...\n",
    "\n",
    "To obtain the 2P for this task, you need to experiment with all three parameters from set **A** and at least three parameters from set **B**. Report your top 10 parameter sets and their loss/accuracy on the dev set. Then, report loss and accuracy of your best parameter set on the test set.\n",
    "\n",
    "Hints:\n",
    "* Activation functions and hidden layer dimensions can also be chosen differently for each layer.\n",
    "* When varying the loss function or the activation functions, it might make sense to redefine the truth labels in the datasets from 0 for NEGative and 1 for POSitive to something else (think of the co-domain of activation functions).\n",
    "\n",
    "### Task 2.3 MLP in Keras (2P)\n",
    "[Install Keras](https://keras.io/#installation) on your machine and implement an MLP with Keras. Hardcode the initial hyperparameter configuration mentioned in Task 2.1 and report loss and accuracy on the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "## Task 1 Backpropagation (4P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\mathbf{r} = max(0,\\mathbf{x} \\mathbf{T}) =max(0, (-1, 1) \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}) = (0.61, 0.64) $ \n",
    "    \n",
    "$\\mathbf{q} = tanh(\\mathbf{r} \\mathbf{U}) = tanh((0.61, 0.64)\\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}) = tanh(-0.5879) = -0.67$\n",
    "\n",
    "$\\mathbf{p}=sig(\\mathbf{q} \\mathbf{V})=sig(-0.67\\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-1.005 & 0.5427 & 0.1608\\\\\n",
    "\\end{pmatrix}) =\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}  $\n",
    "    \n",
    "$\\mathbf{y}=sig(\\mathbf{p}\\mathbf{W}=\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-2.22 & -1.68\\end{pmatrix})=\\begin{pmatrix} 0.098 & 0.16\\end{pmatrix}$\n",
    "    \n",
    "$\\mathbf{E}=(\\mathbf{t} - \\mathbf{y})^2=\\begin{pmatrix} 0.204 & 1.198 \\end{pmatrix} $\n",
    "\n",
    "$\\frac{d \\mathbf{E}}{dy} =2(\\mathbf{t} - \\mathbf{y}) =\\begin{pmatrix} 0.408 &  2.396 \\end{pmatrix}$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 & 2.2 MLP in Tensorflow & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import progressbar as pb\n",
    "\n",
    "label_pos = 1\n",
    "label_neg = 0\n",
    "\n",
    "data_dim = 100\n",
    "data_dim_with_bias = data_dim + 1\n",
    "\n",
    "# learning_rate = 0.1\n",
    "learning_rate = 0.01\n",
    "# learning_rate = 0.001\n",
    "batch_size = 5\n",
    "# batch_size = 10\n",
    "# batch_size = 20\n",
    "# epochs = 50\n",
    "epochs = 100\n",
    "# epochs = 150\n",
    "\n",
    "def read_dataset(src):\n",
    "    \"\"\"Reads a dataset from the specified path and returns input vectors and labels in an array of shape (n, 101).\"\"\"\n",
    "    with open(src, 'r') as src_file:\n",
    "        # preallocate memory for the data\n",
    "        num_lines = sum(1 for line in src_file)\n",
    "        data = np.empty((num_lines, data_dim_with_bias), dtype=np.float16)\n",
    "        labels = np.empty((num_lines, 1), dtype=np.float16)\n",
    "\n",
    "        # reset the file pointer to the beginning of the file\n",
    "        src_file.seek(0)\n",
    "        for i, line in enumerate(src_file):\n",
    "            _, str_label, str_vec = line.split('\\t')\n",
    "            labels[i] = label_pos if str_label.split('=')[1] == \"POS\" else label_neg\n",
    "            data[i,:data_dim] = [float(f) for f in str_vec.split()]\n",
    "            data[i,data_dim] = 1\n",
    "    return data, labels\n",
    "\n",
    "def get_dataset(src_folder, name=\"train\"):\n",
    "    path = os.path.join(src_folder, \"rt-polarity.{}.vecs\".format(name))\n",
    "    return read_dataset(path)\n",
    "\n",
    "def get_random_batches(X, y, batch_size):\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    # when using array_split for 100 datapoints and batch size 33 one would get batches [33, 33, 33, 1]\n",
    "    X_batches = np.array_split(X, len(y)//batch_size)\n",
    "    y_batches = np.array_split(y, len(y)//batch_size)\n",
    "    return X_batches, y_batches\n",
    "\n",
    "# load the data\n",
    "train_x, train_y, = get_dataset(\"DATA\", \"train\")\n",
    "dev_x, dev_y = get_dataset(\"DATA\", \"dev\")\n",
    "test_x, test_y = get_dataset(\"DATA\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 49.7%\n",
      "Loss on test-set: 0.50344\n",
      "Accuracy on dev-set: 50.9%\n",
      "Loss on dev-set: 0.490932\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_input = data_dim_with_bias\n",
    "n_hidden_1 = 10\n",
    "n_hidden_2 = 8\n",
    "\n",
    "# define weights\n",
    "def init_weights(n_input,n_hidden_1,n_hidden_2):\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal(shape=[n_input,n_hidden_1],mean=0,stddev=1)),\n",
    "        'h2': tf.Variable(tf.random_normal(shape=[n_hidden_1,n_hidden_2],mean=0,stddev=1)),\n",
    "        'out': tf.Variable(tf.random_normal(shape=[n_hidden_2,1],mean=0,stddev=1))}\n",
    "    return weights\n",
    "\n",
    "# define model\n",
    "def MLP(X, weights):\n",
    "    # first hidden layer\n",
    "    layer_1 = tf.tanh(tf.matmul(X,weights['h1']))\n",
    "#     layer_1 = tf.nn.relu(tf.matmul(X,weights['h1']))\n",
    "    # second hidden layer\n",
    "    layer_2 = tf.tanh(tf.matmul(layer_1,weights['h2']))\n",
    "#     layer_2 = tf.nn.relu(tf.matmul(layer_1,weights['h2']))\n",
    "    # output\n",
    "    out = tf.nn.softmax(tf.matmul(layer_2,weights['out']))\n",
    "#     out = tf.sigmoid(tf.matmul(layer_2, weights['out']))\n",
    "    return out\n",
    "\n",
    "# define tf graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_input))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# initialize weights\n",
    "weights = init_weights(n_input,n_hidden_1,n_hidden_2)\n",
    "\n",
    "# calculate prediction\n",
    "pred = MLP(X,weights)\n",
    "\n",
    "# calculate cost function and optimization \n",
    "# cost = tf.losses.mean_squared_error(y,pred)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for m in range(epochs):\n",
    "        train_x_batches, train_y_batches = get_random_batches(train_x, train_y, batch_size)\n",
    "        for X_batch, y_batch in zip(train_x_batches, train_y_batches):\n",
    "            sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    #print results on dev and test\n",
    "    correct_prediction = tf.equal(tf.round(pred),y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    loss = tf.losses.mean_squared_error(y,pred)\n",
    "   \n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(accuracy.eval({X:test_x,y:test_y})))\n",
    "    print(\"Loss on test-set:\",loss.eval({X:test_x,y:test_y}))\n",
    "    print(\"Accuracy on dev-set: {0:.1%}\".format(accuracy.eval({X:dev_x,y:dev_y})))\n",
    "    print(\"Loss on dev-set:\",loss.eval({X:dev_x,y:dev_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Top 10 parameter sets:\n",
    "| TOP 10 | batch size | learn rate | epochs | Loss function | # of hidden layers | Activation fucntion | Relularization | Optimizer | Accuracy | Loss        \n",
    "| :- |-------------: | :-:\n",
    "| 1 | 5 | 0.01 | 100 | square loss | 10, 8 | tanh | No |SGD |  68.5% | 0.217952\n",
    "| 2 | 5 | 0.01 | 100 | square loss | 10, 10 | tanh | No |SGD | 66.2% | 0.220211\n",
    "| 3 | 5 | 0.01 | 100 | square loss | 10, 5 | tanh | No |SGD | 66.2% | 0.226095\n",
    "| 4 | 5 | 0.01 | 200 | square loss | 10, 10 | tanh | No |SGD | 65.7% | 0.213632\n",
    "| 5 | 5 | 0.01 | 100 | square loss | 10, 8 | tanh & softplus | No | SGD | 65.3% | 0.215742\n",
    "| 6 | 10 | 0.01 | 100 | square loss | 10, 10 | tanh | No |SGD | 62.7% | 0.228515\n",
    "| 7 | 10 | 0.1 | 100 | square loss | 10, 10 | tanh | No |SGD | 61.5% | 0.232456\n",
    "| 8 | 20 | 0.01 | 100 | square loss | 10, 10 | tanh | No |SGD | 59.2% | 0.251632\n",
    "| 9 | 5 | 0.01 | 50 | square loss | 10, 10 | tanh | No |SGD | 58.3% | 0.247174\n",
    "| 10 | 5 | 0.01 | 100 | cross entropy loss | 10, 8 | softmax | No |SGD | 50.9% | 0.490932\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameter on test-sets:\n",
    "Accuracy on test-set: 67.3% --- Loss on test-set: 0.21821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Results\n",
    "Accuracy on test-set: 63.0% --- Loss on test-set: 0.2239\n",
    "\n",
    "Accuracy on dev-set: 62.7% --- Loss on dev-set: 0.228515\n",
    "\n",
    "###  When learning_rate= 0.001\n",
    "Accuracy on test-set: 55.3% --- Loss on test-set: 0.250549\n",
    "\n",
    "Accuracy on dev-set: 52.3% --- Loss on dev-set: 0.258009\n",
    "\n",
    "###  When learning_rate= 0.1  \n",
    "Accuracy on test-set: 62.9% --- Loss on test-set: 0.226974\n",
    "\n",
    "Accuracy on dev-set: 61.5% --- Loss on dev-set: 0.232456\n",
    "\n",
    "### When batch_size= 5\n",
    "Accuracy on test-set: 67.4% --- Loss on test-set: 0.208212\n",
    "\n",
    "Accuracy on dev-set: 66.2% --- Loss on dev-set: 0.220211\n",
    "\n",
    "\n",
    "### When batch_size= 20\n",
    "Accuracy on test-set: 59.7% --- Loss on test-set: 0.2413\n",
    "\n",
    "Accuracy on dev-set: 59.2% --- Loss on dev-set: 0.251632\n",
    "\n",
    "### When epochs= 50\n",
    "Accuracy on test-set: 56.7% --- Loss on test-set: 0.25013\n",
    "\n",
    "Accuracy on dev-set: 58.3% --- Loss on dev-set: 0.247174\n",
    "\n",
    "### When epochs=200\n",
    "Accuracy on test-set: 65.5% --- Loss on test-set: 0.214483\n",
    "\n",
    "Accuracy on dev-set: 65.7% --- Loss on dev-set: 0.213632\n",
    "\n",
    "### When hidden layer dimension: 10, 5\n",
    "Accuracy on test-set: 67.0% --- Loss on test-set: 0.22078\n",
    "\n",
    "Accuracy on dev-set: 66.2% --- Loss on dev-set: 0.226095\n",
    "\n",
    "### When hidden layer diemension: 10, 8\n",
    "Accuracy on test-set: 67.3% --- Loss on test-set: 0.21821\n",
    "\n",
    "Accuracy on dev-set: 68.5% --- Loss on dev-set: 0.217952\n",
    "\n",
    "### Activation: ReLU, ReLU, tanh\n",
    "Accuracy on test-set: 49.5% --- Loss on test-set: 0.509068\n",
    "\n",
    "Accuracy on dev-set: 50.8% --- Loss on dev-set: 0.49531\n",
    "\n",
    "### Activation: tanh, tanh, softplus\n",
    "Accuracy on test-set: 65.5% --- Loss on test-set: 0.220412\n",
    "\n",
    "Accuracy on dev-set: 65.3% --- Loss on dev-set: 0.215742\n",
    "\n",
    "### Loss function: softmax+cross-entropy\n",
    "Accuracy on test-set: 49.7% --- Loss on test-set: 0.50344\n",
    "\n",
    "Accuracy on dev-set: 50.9% --- Loss on dev-set: 0.490932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 MLP in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6870 - acc: 0.5588     \n",
      "Epoch 2/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6537 - acc: 0.6085     \n",
      "Epoch 3/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6324 - acc: 0.6400     \n",
      "Epoch 4/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6203 - acc: 0.6526     \n",
      "Epoch 5/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6061 - acc: 0.6652     \n",
      "Epoch 6/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6035 - acc: 0.6736     \n",
      "Epoch 7/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6028 - acc: 0.6677     \n",
      "Epoch 8/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6026 - acc: 0.6748     \n",
      "Epoch 9/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6011 - acc: 0.6760     \n",
      "Epoch 10/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5936 - acc: 0.6787     \n",
      "Epoch 11/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5907 - acc: 0.6740     \n",
      "Epoch 12/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5921 - acc: 0.6868     \n",
      "Epoch 13/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6000 - acc: 0.6856     \n",
      "Epoch 14/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6040 - acc: 0.6719     \n",
      "Epoch 15/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.6010 - acc: 0.6759     \n",
      "Epoch 16/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5932 - acc: 0.6927     \n",
      "Epoch 17/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5960 - acc: 0.6720     - ETA: 0s - loss: 0.569\n",
      "Epoch 18/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5923 - acc: 0.6752     \n",
      "Epoch 19/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5865 - acc: 0.6927     \n",
      "Epoch 20/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5894 - acc: 0.6823     \n",
      "Epoch 21/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5847 - acc: 0.6838     \n",
      "Epoch 22/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5721 - acc: 0.7061     \n",
      "Epoch 23/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5729 - acc: 0.6960     \n",
      "Epoch 24/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5695 - acc: 0.7010     \n",
      "Epoch 25/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5772 - acc: 0.6965     \n",
      "Epoch 26/100\n",
      "7464/7464 [==============================] - 1s - loss: 0.5845 - acc: 0.6763     \n",
      "Epoch 27/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5756 - acc: 0.6955     \n",
      "Epoch 28/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5702 - acc: 0.7016     \n",
      "Epoch 29/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5773 - acc: 0.6890     \n",
      "Epoch 30/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5721 - acc: 0.7008     \n",
      "Epoch 31/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5727 - acc: 0.6902     \n",
      "Epoch 32/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5703 - acc: 0.7101     \n",
      "Epoch 33/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5717 - acc: 0.6991     \n",
      "Epoch 34/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5848 - acc: 0.6746     \n",
      "Epoch 35/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5670 - acc: 0.7091     \n",
      "Epoch 36/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5677 - acc: 0.7038     \n",
      "Epoch 37/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5538 - acc: 0.7165     \n",
      "Epoch 38/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5618 - acc: 0.7040     \n",
      "Epoch 39/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5653 - acc: 0.7044     \n",
      "Epoch 40/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5608 - acc: 0.6998     \n",
      "Epoch 41/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5567 - acc: 0.7046     \n",
      "Epoch 42/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5683 - acc: 0.7035     \n",
      "Epoch 43/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5794 - acc: 0.6999     \n",
      "Epoch 44/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5752 - acc: 0.6919     \n",
      "Epoch 45/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5696 - acc: 0.7014     \n",
      "Epoch 46/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5635 - acc: 0.7051     \n",
      "Epoch 47/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5678 - acc: 0.7019     \n",
      "Epoch 48/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5740 - acc: 0.6890     \n",
      "Epoch 49/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5735 - acc: 0.6953     \n",
      "Epoch 50/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5662 - acc: 0.6973     \n",
      "Epoch 51/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5871 - acc: 0.6932     \n",
      "Epoch 52/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5829 - acc: 0.6702     \n",
      "Epoch 53/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5844 - acc: 0.6534     \n",
      "Epoch 54/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5744 - acc: 0.6888     \n",
      "Epoch 55/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5759 - acc: 0.6992     \n",
      "Epoch 56/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5750 - acc: 0.7003     \n",
      "Epoch 57/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5858 - acc: 0.6877     \n",
      "Epoch 58/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5630 - acc: 0.7059     \n",
      "Epoch 59/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5607 - acc: 0.6909     \n",
      "Epoch 60/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5824 - acc: 0.6885     \n",
      "Epoch 61/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5723 - acc: 0.6756     \n",
      "Epoch 62/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5723 - acc: 0.6972     \n",
      "Epoch 63/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5745 - acc: 0.6900     \n",
      "Epoch 64/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5750 - acc: 0.6830     \n",
      "Epoch 65/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5760 - acc: 0.7077     \n",
      "Epoch 66/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5786 - acc: 0.6811     \n",
      "Epoch 67/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5759 - acc: 0.6964     \n",
      "Epoch 68/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5681 - acc: 0.6873     \n",
      "Epoch 69/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5599 - acc: 0.7075     \n",
      "Epoch 70/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5586 - acc: 0.6983     \n",
      "Epoch 71/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5570 - acc: 0.7141     \n",
      "Epoch 72/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5533 - acc: 0.7121     \n",
      "Epoch 73/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5468 - acc: 0.7176     \n",
      "Epoch 74/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5449 - acc: 0.7178     \n",
      "Epoch 75/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5521 - acc: 0.7086     \n",
      "Epoch 76/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5561 - acc: 0.7126     \n",
      "Epoch 77/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5541 - acc: 0.7044     \n",
      "Epoch 78/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5668 - acc: 0.6763     \n",
      "Epoch 79/100\n",
      "7464/7464 [==============================] - 1s - loss: 0.5641 - acc: 0.6917     \n",
      "Epoch 80/100\n",
      "7464/7464 [==============================] - 1s - loss: 0.5564 - acc: 0.7061     \n",
      "Epoch 81/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5549 - acc: 0.7075     \n",
      "Epoch 82/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5465 - acc: 0.7191     \n",
      "Epoch 83/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5544 - acc: 0.7109     \n",
      "Epoch 84/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5537 - acc: 0.7156     \n",
      "Epoch 85/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5566 - acc: 0.7087     \n",
      "Epoch 86/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5494 - acc: 0.7043     \n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7464/7464 [==============================] - 0s - loss: 0.5457 - acc: 0.7142     \n",
      "Epoch 88/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5401 - acc: 0.7244     \n",
      "Epoch 89/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5505 - acc: 0.7121     \n",
      "Epoch 90/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5445 - acc: 0.7207     \n",
      "Epoch 91/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5460 - acc: 0.7231     \n",
      "Epoch 92/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5474 - acc: 0.7169     \n",
      "Epoch 93/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5446 - acc: 0.7168     \n",
      "Epoch 94/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5656 - acc: 0.7004     \n",
      "Epoch 95/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5602 - acc: 0.6858     \n",
      "Epoch 96/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5584 - acc: 0.7098     \n",
      "Epoch 97/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5548 - acc: 0.7087     \n",
      "Epoch 98/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5520 - acc: 0.7223     \n",
      "Epoch 99/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5435 - acc: 0.7197     \n",
      "Epoch 100/100\n",
      "7464/7464 [==============================] - 0s - loss: 0.5421 - acc: 0.7213     \n",
      " 990/1599 [=================>............] - ETA: 0sAccuracy on dev-set: 70.04%\n",
      "Loss aon dev-set: 0.571630382515\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "n_hidden_1 = 10\n",
    "n_hidden_2 = 8\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(n_hidden_1,activation='tanh', input_dim=data_dim_with_bias))\n",
    "# model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Dense(n_hidden_2,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y,epochs=epochs,batch_size=batch_size)\n",
    "scores = model.evaluate(dev_x,dev_y,batch_size=batch_size)\n",
    "print(\"Accuracy on dev-set: %.2f%%\" % (scores[1]*100))  # accuracy\n",
    "print(\"Loss on dev-set:\",scores[0]) # Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, Accuracy on dev-set: 70.04%, Loss on dev-set: 0.571630382515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
