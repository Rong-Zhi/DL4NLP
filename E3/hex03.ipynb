{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 03\n",
    "----------------------------------\n",
    "**Due until Tuesday, 09.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Backpropagation (4P)\n",
    "\n",
    "![MLP](backprop_hex03_task.png)\n",
    "\n",
    "#### Network details\n",
    "\n",
    "The activation function used in each layer is shown above the respective layer.\n",
    "\n",
    "The network loss is square loss:\n",
    "\\begin{equation}\n",
    "    \\ell(t, y) = (t - y)^2 \\qquad \\ell'(t, y) = 2(y-t)\n",
    "\\end{equation}\n",
    "$t$ denotes the true label, $y$ denotes the network output.\n",
    "\n",
    "The weight matrices are initialized as follows:\n",
    "\\begin{equation}\n",
    "\t\\mathbf{T} = \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{U} = \\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{V} = \\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\t\\quad\n",
    "\t\\mathbf{W} = \\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Task 1.1 Backpropagation by hand (4P)\n",
    "Using pen and paper, perform one execution of backpropagation on multilayer perceptron given above. \"One execution\" includes the forward propagation step, the backwards propagation and weight updates.\n",
    "Use $\\mathbf{x} = (-1, 1)$ as the input and $\\mathbf{t} = (0,1)$ as the output. Use $\\alpha=0.75$ for the learning rate.\n",
    "\n",
    "The necessary formulas can be found on slides 11-13 [in the exercise slides](https://moodle.informatik.tu-darmstadt.de/mod/resource/view.php?id=7422). Round your results to four decimal points after each calculation.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "Please show your process of calculation. Both LaTeX solutions and scanned sheets are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Sentiment Polarity in Movie Reviews (6P)\n",
    "We revisit last week's task of identifying sentiments in movie reviews.\n",
    "\n",
    "#### Data (same as last week)\n",
    "In the `hex03_data` archive, you can find a training, development and test dataset. Each line in these datasets has three entries, separated by a tab character (`\\t`). The first is the movie review (available only for reference), the second is the sentiment label (`POS`itive or `NEG`ative). To facilitate the task, the third entry is a 100-dimensional vector representing the review (we'll cover in later lectures on *word embeddings* how this sentence representation has been generated).\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "\n",
    "\n",
    "### Task 2.0 Dataset reader (0P)\n",
    "To read the datasets, reuse your own reader from last week or copy the one from the solution provided in moodle.\n",
    "\n",
    "### Task 2.1 MLP in TensorFlow (2P)\n",
    "\n",
    "Implement a multilayer perceptron in TensorFlow. Try to keep it as configurable as possible for the hyperparameter optimization for task 2.2.\n",
    "\n",
    "Report loss and accuracy on the dev set for the following configuration:\n",
    "* square loss\n",
    "* 2 hidden layers, both with a dimension of 10 and using tanh activation\n",
    "* weights initialized by the standard normal distribution (mean 0, sigma 1)\n",
    "* batch size 10\n",
    "* learning rate $\\alpha = 0.01$\n",
    "* 100 epochs\n",
    "\n",
    "\n",
    "### Task 2.2 Hyperparameter Optimization (2P)\n",
    "In lecture 02, slide 19ff you learned about random hyperparameter optimization. Try to find the best configuration for your MLP from task 2.1 by experimenting with the following hyperparameters:\n",
    "\n",
    "* **A**: Known from the previous home exercise:\n",
    "    * batch size\n",
    "    * learning rate\n",
    "    * number of training epochs\n",
    "\n",
    "\n",
    "* **B**: New parameters:\n",
    "    * loss function: square loss, hinge loss, softmax + cross-entropy loss\n",
    "    * number of hidden layers: anything values from the interval $[0, \\infty[$\n",
    "    * hidden layer dimension: anything values from the interval $[1, \\infty[$\n",
    "    * activation function(s): sigmoid, tanh, ReLU, softplus, leaky ReLU, ELU, maxout\n",
    "    * 2-norm regularization of hidden layer weights (see lecture 03, slide 37): yes/no\n",
    "    * optimizer: SGD, Adam, AdaGrad, ...\n",
    "\n",
    "To obtain the 2P for this task, you need to experiment with all three parameters from set **A** and at least three parameters from set **B**. Report your top 10 parameter sets and their loss/accuracy on the dev set. Then, report loss and accuracy of your best parameter set on the test set.\n",
    "\n",
    "Hints:\n",
    "* Activation functions and hidden layer dimensions can also be chosen differently for each layer.\n",
    "* When varying the loss function or the activation functions, it might make sense to redefine the truth labels in the datasets from 0 for NEGative and 1 for POSitive to something else (think of the co-domain of activation functions).\n",
    "\n",
    "### Task 2.3 MLP in Keras (2P)\n",
    "[Install Keras](https://keras.io/#installation) on your machine and implement an MLP with Keras. Hardcode the initial hyperparameter configuration mentioned in Task 2.1 and report loss and accuracy on the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "## Task 1 Backpropagation (4P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\mathbf{r} = max(0,\\mathbf{x} \\mathbf{T}) =max(0, (-1, 1) \\begin{pmatrix}\n",
    "\t\t0.19 & -0.92  \\\\\n",
    "\t\t-0.42 & -0.28 \\\\\n",
    "\t\\end{pmatrix}) = (0.61, 0.64) $ \n",
    "    \n",
    "$\\mathbf{q} = tanh(\\mathbf{r} \\mathbf{U}) = tanh((0.61, 0.64)\\begin{pmatrix}\n",
    "\t\t0.61 \\\\\n",
    "\t\t-1.5 \\\\\n",
    "\t\\end{pmatrix}) = tanh(-0.5879) = -0.67$\n",
    "\n",
    "$\\mathbf{p}=sig(\\mathbf{q} \\mathbf{V})=sig(-0.67\\begin{pmatrix}\n",
    "\t\t1.5 & -0.81 & -0.24 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-1.005 & 0.5427 & 0.1608\\\\\n",
    "\\end{pmatrix}) =\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}  $\n",
    "    \n",
    "$\\mathbf{y}=sig(\\mathbf{p}\\mathbf{W}=\\begin{pmatrix} 0.27 &  0.63 & 0.54 \\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\t\t-1.4 & -0.81 \\\\\n",
    "\t\t-2.2 & -1.7 \\\\\n",
    "\t\t-0.27 & -0.73 \\\\\n",
    "\t\\end{pmatrix})=sig(\\begin{pmatrix}-2.22 & -1.68\\end{pmatrix})=\\begin{pmatrix} 0.098 & 0.16\\end{pmatrix}$\n",
    "    \n",
    "$\\mathbf{E}=(\\mathbf{t} - \\mathbf{y})^2=\\begin{pmatrix} 0.204 & 1.198 \\end{pmatrix} $\n",
    "\n",
    "$\\frac{d \\mathbf{E}}{dy} =2(\\mathbf{t} - \\mathbf{y}) =\\begin{pmatrix} 0.408 &  2.396 \\end{pmatrix}$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 & 2.2 MLP in Tensorflow & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import progressbar as pb\n",
    "\n",
    "label_pos = 1\n",
    "label_neg = 0\n",
    "\n",
    "data_dim = 100\n",
    "data_dim_with_bias = data_dim + 1\n",
    "\n",
    "# learning_rate = 0.1\n",
    "learning_rate = 0.01\n",
    "# learning_rate = 0.001\n",
    "batch_size = 5\n",
    "# batch_size = 10\n",
    "# batch_size = 20\n",
    "# epochs = 50\n",
    "epochs = 100\n",
    "# epochs = 150\n",
    "\n",
    "def read_dataset(src):\n",
    "    \"\"\"Reads a dataset from the specified path and returns input vectors and labels in an array of shape (n, 101).\"\"\"\n",
    "    with open(src, 'r') as src_file:\n",
    "        # preallocate memory for the data\n",
    "        num_lines = sum(1 for line in src_file)\n",
    "        data = np.empty((num_lines, data_dim_with_bias), dtype=np.float16)\n",
    "        labels = np.empty((num_lines, 1), dtype=np.float16)\n",
    "\n",
    "        # reset the file pointer to the beginning of the file\n",
    "        src_file.seek(0)\n",
    "        for i, line in enumerate(src_file):\n",
    "            _, str_label, str_vec = line.split('\\t')\n",
    "            labels[i] = label_pos if str_label.split('=')[1] == \"POS\" else label_neg\n",
    "            data[i,:data_dim] = [float(f) for f in str_vec.split()]\n",
    "            data[i,data_dim] = 1\n",
    "    return data, labels\n",
    "\n",
    "def get_dataset(src_folder, name=\"train\"):\n",
    "    path = os.path.join(src_folder, \"rt-polarity.{}.vecs\".format(name))\n",
    "    return read_dataset(path)\n",
    "\n",
    "def get_random_batches(X, y, batch_size):\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    # when using array_split for 100 datapoints and batch size 33 one would get batches [33, 33, 33, 1]\n",
    "    X_batches = np.array_split(X, len(y)//batch_size)\n",
    "    y_batches = np.array_split(y, len(y)//batch_size)\n",
    "    return X_batches, y_batches\n",
    "\n",
    "# load the data\n",
    "train_x, train_y, = get_dataset(\"DATA\", \"train\")\n",
    "dev_x, dev_y = get_dataset(\"DATA\", \"dev\")\n",
    "test_x, test_y = get_dataset(\"DATA\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 0.0%\n",
      "Loss on test-set: 1.18901e+08\n",
      "Accuracy on dev-set: 0.0%\n",
      "Loss on dev-set: 1.19025e+08\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_input = data_dim_with_bias\n",
    "n_hidden_1 = 10\n",
    "n_hidden_2 = 8\n",
    "\n",
    "# define weights\n",
    "def init_weights(n_input,n_hidden_1,n_hidden_2):\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal(shape=[n_input,n_hidden_1],mean=0,stddev=1)),\n",
    "        'h2': tf.Variable(tf.random_normal(shape=[n_hidden_1,n_hidden_2],mean=0,stddev=1)),\n",
    "        'out': tf.Variable(tf.random_normal(shape=[n_hidden_2,1],mean=0,stddev=1))}\n",
    "    return weights\n",
    "\n",
    "# define model\n",
    "def MLP(X, weights):\n",
    "    # first hidden layer\n",
    "    layer_1 = tf.tanh(tf.matmul(X,weights['h1']))\n",
    "#     layer_1 = tf.nn.relu(tf.matmul(X,weights['h1']))\n",
    "    # second hidden layer\n",
    "    layer_2 = tf.tanh(tf.matmul(layer_1,weights['h2']))\n",
    "#     layer_2 = tf.nn.relu(tf.matmul(layer_1,weights['h2']))\n",
    "    # output\n",
    "    out = tf.matmul(layer_2,weights['out'])\n",
    "#     out = tf.sigmoid(tf.matmul(layer_2, weights['out']))\n",
    "    return out\n",
    "\n",
    "# define tf graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_input))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# initialize weights\n",
    "weights = init_weights(n_input,n_hidden_1,n_hidden_2)\n",
    "\n",
    "# calculate prediction\n",
    "pred = MLP(X,weights)\n",
    "\n",
    "# calculate cost function and optimization \n",
    "# cost = tf.losses.mean_squared_error(y,pred)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for m in range(epochs):\n",
    "        train_x_batches, train_y_batches = get_random_batches(train_x, train_y, batch_size)\n",
    "        for X_batch, y_batch in zip(train_x_batches, train_y_batches):\n",
    "            sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    #print results on dev and test\n",
    "    correct_prediction = tf.equal(tf.round(pred),y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    loss = tf.losses.mean_squared_error(y,pred)\n",
    "   \n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(accuracy.eval({X:test_x,y:test_y})))\n",
    "    print(\"Loss on test-set:\",loss.eval({X:test_x,y:test_y}))\n",
    "    print(\"Accuracy on dev-set: {0:.1%}\".format(accuracy.eval({X:dev_x,y:dev_y})))\n",
    "    print(\"Loss on dev-set:\",loss.eval({X:dev_x,y:dev_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Initial parameters:\n",
    "batch_size = 10; \n",
    "\n",
    "learning_rate= 0.01;\n",
    "\n",
    "epochs= 100;\n",
    "\n",
    "Loss function: square loss; \n",
    "\n",
    "#of hidden layers: 10, 10; \n",
    "\n",
    "Activation function: tanh;\n",
    "\n",
    "Regularization of hidden layers: No; \n",
    "\n",
    "optimizer: SGD\n",
    "### Results\n",
    "Accuracy on test-set: 63.0% --- Loss on test-set: 0.2239\n",
    "\n",
    "Accuracy on dev-set: 62.7% --- Loss on dev-set: 0.228515\n",
    "\n",
    "###  When learning_rate= 0.001\n",
    "Accuracy on test-set: 55.3% --- Loss on test-set: 0.250549\n",
    "\n",
    "Accuracy on dev-set: 52.3% --- Loss on dev-set: 0.258009\n",
    "\n",
    "###  When learning_rate= 0.1  \n",
    "Accuracy on test-set: 62.9% --- Loss on test-set: 0.226974\n",
    "\n",
    "Accuracy on dev-set: 61.5% --- Loss on dev-set: 0.232456\n",
    "\n",
    "### When batch_size= 5\n",
    "Accuracy on test-set: 67.4% --- Loss on test-set: 0.208212\n",
    "\n",
    "Accuracy on dev-set: 66.2% --- Loss on dev-set: 0.220211\n",
    "\n",
    "\n",
    "### When batch_size= 20\n",
    "Accuracy on test-set: 59.7% --- Loss on test-set: 0.2413\n",
    "\n",
    "Accuracy on dev-set: 59.2% --- Loss on dev-set: 0.251632\n",
    "\n",
    "### When epochs= 50\n",
    "Accuracy on test-set: 56.7% --- Loss on test-set: 0.25013\n",
    "\n",
    "Accuracy on dev-set: 58.3% --- Loss on dev-set: 0.247174\n",
    "\n",
    "### When epochs=200\n",
    "Accuracy on test-set: 65.5% --- Loss on test-set: 0.214483\n",
    "\n",
    "Accuracy on dev-set: 65.7% --- Loss on dev-set: 0.213632\n",
    "\n",
    "### When hidden layer dimension: 10, 5\n",
    "Accuracy on test-set: 67.0% --- Loss on test-set: 0.22078\n",
    "\n",
    "Accuracy on dev-set: 66.2% --- Loss on dev-set: 0.226095\n",
    "\n",
    "### When hidden layer diemension: 10, 8\n",
    "Accuracy on test-set: 67.3% --- Loss on test-set: 0.21821\n",
    "\n",
    "Accuracy on dev-set: 68.5% --- Loss on dev-set: 0.217952\n",
    "\n",
    "### Activation: ReLU, ReLU, tanh\n",
    "Accuracy on test-set: 49.5% --- Loss on test-set: 0.509068\n",
    "\n",
    "Accuracy on dev-set: 50.8% --- Loss on dev-set: 0.49531\n",
    "\n",
    "### Activation: tanh, tanh, softplus\n",
    "Accuracy on test-set: 65.5% --- Loss on test-set: 0.220412\n",
    "\n",
    "Accuracy on dev-set: 65.3% --- Loss on dev-set: 0.215742\n",
    "\n",
    "### Loss function: softmax+cross-entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
