{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DL4NLP SS17 Home Exercise 02\n",
    "----------------------------------\n",
    "**Due until Tuesday, 02.05. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 1 Mandatory Video (1P)\n",
    "Watch [this week's mandatory video](https://www.youtube.com/watch?v=GUtlrDbHhJM&t=277s&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&index=5) and answer the following question: Which six activation functions are discussed during the talk?\n",
    "\n",
    "Hint: They are shown on a summary slide among the first 30 slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 2 Sentiment Polarity in Movie Reviews (9P)\n",
    "The movie-review dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/) consists of movie reviews labeled with sentiment polarity (i.e. \"positive review\" or \"negative review\"). Your task is to implement a variation of the perceptron from last week which learns to identify the sentiment in a review.\n",
    "\n",
    "#### Data\n",
    "In the `hex02_data` archive, you can find a training, development and test dataset. Each line in these datasets has three entries, separated by a tab character (`\\t`). The first is the movie review (available only for reference), the second is the sentiment label (`POS`itive or `NEG`ative). To facilitate the task, the third entry is a 100-dimensional vector representing the review (we'll cover in later lectures on *word embeddings* how this sentence representation has been generated).\n",
    "\n",
    "#### Perceptron\n",
    "* As the loss function, choose square-loss:\n",
    "\\begin{equation}\n",
    "    L = \\sum_{j=1}^N \\ell(\\mathbf{x}_j, y) = \\sum_{j=1}^N (\\sigma(\\mathbf{x}_j \\cdot \\mathbf{w}) - y_j)^2\n",
    "\\end{equation}\n",
    "* For the activation function, use the sigmoid function.\n",
    "\n",
    "* For the weight update rule, use the following mini-batch stochastic gradient descent formula:\n",
    "\\begin{equation}\n",
    "    w' \\leftarrow w - \\frac{\\alpha}{|\\mathcal{T}'|} \\cdot \\sum_{(\\mathbf{x},y)\\in\\mathcal{T}'} \\Bigl(\\sigma(\\mathbf{x} \\cdot \\mathbf{w}) - y\\Bigr) \\cdot \\sigma'(\\mathbf{x} \\cdot \\mathbf{w}) \\cdot x^T\n",
    "\\end{equation}\n",
    "Reminder: $\\mathcal{T'}$ is a mini-batch; a random subset of the whole training data $\\mathcal{T}$. A typical way of implementing random mini-batches is to randomly shuffle the whole training dataset before each epoch, then divide the training dataset into batches of size $|\\mathcal{T'}|$. Consider setting the numpy random seed for reproducible results.\n",
    "\n",
    "* Use the 100-dimensional vectors from the datasets for the input vectors $\\mathbf{x}$. Encode the corresponding label as $y=1$ for `POS` and $y=0$ for `NEG` (i.e. according to the co-domain of the sigmoid activation function). **Append a trailing 1 to each input vector $\\mathbf{x}$ for the bias (cf. lecture 01, slide 92).**\n",
    "\n",
    "* Initialize the weight vector via `w=np.random.normal(0,1,(N,1))`, where $N$ is the dimensionality of your input data.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "\n",
    "\n",
    "### Task 2.1 Dataset reader (1P)\n",
    "Implement a reader for the dataset files which returns the input vectors $\\mathbf{x}$ and labels $y$ as numpy arrays. The shape and number of returned arrays is up to you.\n",
    "\n",
    "### Task 2.2 Numpy implementation (5P)\n",
    "\n",
    "a) Implement the perceptron stated above only using numpy. Include a method which computes the square loss and the accuracy of the model, given a dataset and a weight vector `w`. (3P)\n",
    "\n",
    "Hint: In order to compute the accuracy, you need to find a meaningful way to interpret your perceptron's prediction $\\sigma(\\mathbf{x} \\cdot \\mathbf{w})$ for a given test input $\\mathbf{x}$ and trained weights $\\mathbf{w}$.\n",
    "\n",
    "b) Train your perceptron on the training data and observe its accuracy on the **development** set. Start with batch size $|\\mathcal{T}'| = 10$, learning rate $\\alpha = 0.01$ and 100 epochs. Experiment with different values for these three hyperparameters. Can you find a configuration which beats 70% accuracy on the development set? Report your best configuration and both the loss and accuracy it reaches on the **development and test** sets. (1P)\n",
    "\n",
    "c) Create a plot similar to the one from lecture 02, slide 18, i.e. plot the loss on the training set and the development set vs. the number of training epochs. For this purpose, run your perceptron with batch size $|\\mathcal{T}'| = 1$ and learning rate $\\alpha = 0.001$ for a large number of epochs (>2500). Which number of epochs is reasonable? Why does the loss on the development set **not** increase over time, contrary to the figure from the lecture? (1P)\n",
    "\n",
    "Hint: For comparable results in the plot, normalize the loss by the number of instances in the training/test dataset.\n",
    "\n",
    "### Task 2.3 TensorFlow (3P)\n",
    "[Install TensorFlow](https://www.tensorflow.org/install/) on your machine and reimplement the perceptron with TensorFlow. Using the initial hyperparameter configuration mentioned in Task 2.2 b), [compare the runtime](http://stackoverflow.com/a/7370824) for training your TensorFlow and your numpy perceptrons for an average of 10 executions. Which one runs faster on your machine?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
